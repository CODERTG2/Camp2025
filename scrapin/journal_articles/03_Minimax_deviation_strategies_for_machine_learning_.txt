================================================================================
JOURNAL ARTICLE #3
================================================================================

Title: Minimax deviation strategies for machine learning and recognition with
  short learning samples
Authors: Michail Schlesinger, Evgeniy Vodolazskiy
Published: 2017-07-16
Source: http://arxiv.org/pdf/1707.04849v1

--------------------------------------------------------------------------------
ABSTRACT/SUMMARY:
--------------------------------------------------------------------------------
The article is devoted to the problem of small learning samples in machine
learning. The flaws of maximum likelihood learning and minimax learning are
looked into and the concept of minimax deviation learning is introduced that is
free of those flaws.

--------------------------------------------------------------------------------
FULL TEXT CONTENT:
--------------------------------------------------------------------------------
arXiv:1707.04849v1 [cs. LG] 16 Jul 2017Minimax deviation strategies for machine
learning and recognition with short learning
samples
Schlesinger M. I. and Vodolazskiy E. V. August 31, 2018
Abstract
The article is devoted to the problem of small learning sampl es in
machine learning. Theﬂaws of maximum likelihood learning a nd min-
imax learning are looked into and the concept of minimax devi ation
learning is introduced that is free of those ﬂaws.
1 Introduction
The small learning sample problem has been around in machine learning
under diﬀerent names during its whole life. The learning sample is used t o
compensate for the lack of knowledge about the recognized objec t when its
statistical model is not completely known. Naturally, the longer the learning
sample is, the better is the subsequent recognition. However, whe n the learn-
ing sample becomes too small (2, 3, 5 elements) an eﬀect of small sam ples
becomes evident. In spite of the fact that any learning sample (eve n a very
small one) provides some additional information about the object, it may be
better to ignore the learning sample than to utilize it with the commonly
used methods. Example 1. Let us consider an object that can be in one of two random
statesy= 1 and y= 2 with equal probabilities. In each state the object
generates two independent Gaussian randomsignals x1andx2with variances
equal 1. Mean values of signals depend on the state as it is shown on F ig.
1 1. In the ﬁrst state the mean value is (2 ,0). In the second state the mean
value depends on an unknown parameter θand is (0,θ). Even if no learning
sample is given a minimax strategy can be used to make a decision about the
statey. The minimax strategy ignores the second signal and makes decision
y∗= 1 when x1>1 and decision y∗= 2 when x1≤1.
x2x1
/Bullet /Bullet
θ/Bullet
2
0y∗= 1
y∗= 2p(x1,x2|y= 1)
p(x1,x2|y= 2)
Figure 1: Example 1. ( x1,x2)∈R2– signal, y∈ {1,2}– state. Now let us assume that there is a sample of signals generated by an
objectinthesecondstatebutwithhighervariance16. Amaximumlike lihood
strategyestimatestheunknownparameter θandthenmakesadecisionabout
yasiftheestimatedvalueoftheparameterisitstruevalue. Fig. 2sho wshow
the probability of a wrong decision (called the risk) depends on param eterθ
for diﬀerent sizes of the learning sample. If the learning sample is suﬃ ciently
long, the risk of maximum likelihood strategy may become arbitrarily clo se
to the minimum possible risk. Naturally, when the length of the sample
decreases the risk becomes worse and worse. Furthermore, whe n it becomes
as small as 3 or 2 elements the risk of the maximum likelihood strategy
becomes worse than the risk of the minimax strategy that uses neit her the
learning sample nor the signal x2at all. Hence, it is better to ignore available
additional data about the recognized object than to try to make u se of it in
a conventional way. It demonstrates a serious theoretical ﬂaw o f commonly
usedmethods, anddeﬁnitely notthatshortsamples areuseless. A nylearning
sample, no mater how long or short it is, provides some, may be not a lo t
information about the recognized object and a reasonable method has to use
it.
2 θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
n= 1 n= 2
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
n= 3 n= 10
Figure 2: Example 1. Probability of a wrong decision (risk) for diﬀeren t
sizesnof the learning sample. The curve R(qML,θ) is the risk of a maximum
likelihoodstrategy. The curve R(qminmax,θ)istherisk ofaminimax strategy. The curve min
qR(q,θ) is the minimum possible risk for each model. Example 2. This is a simple example that has been used by H. Robbins in
his seminal article [5] where he initiated empirical Bayessian approach and
explaned its main idea. An object can be in one of two possible states y= 1
andy= 2. In each state the object generates a univariate Gaussian sign al
xwith variance 1. The mean value of the generated signal depends on the
stateyso that
p(x|y= 1) =1√
2πe−(x+1)2
2, p(x|y= 2) =1√
2πe−(x−1)2
2. Only a priori probabilities of states are unknown and θis the probability
of the ﬁrst state so that p(y= 1) =θandp(y= 2) = 1 −θ. A minimax
strategy for such incomplete statistical model makes decision y∗based on
the sign of the observed signal and ensures probability of correct recognition
0.84 independently of a priori probabilities of states. Let not only a single object, but a collection of mutually independant
objectsbeavailableforrecognition. Eachobjectisinitsownhiddens tateand
is presented with its own signal. Let us also assume that the decision a bout
3 xy∗= 2 y∗= 1p(x|y= 1) p(x|y= 2)
/Bullet
1/Bullet
−1/Bullet
Figure 3: Example 2. x∈R– signal, y∈ {1,2}– state.
each object’s state does not have to be made immediately when the o bject
is observed and can be postponed until the whole collection is observ ed. In
this case maximum likelihood estimations of a priori probabilities of stat es
can be computed and then each object of the collection is recognize d as if the
estimated values of probabilities were the true values. When the pre sented
collection is suﬃciently long the probability of a wrong decision can be ma de
as close to the minimum as possible (Fig.4). However, when the collect ion is
too short, the probability of a wrong decision can be much worse tha n that
of the minimax strategy. The considered examples lead to a diﬃcult and up to now an unanswere d
question. What should be done when a ﬁxed sample of 2-3 elements is g iven
and no additional elements can be obtained? Is it really the best way t o
ignore these data or is it possible to make use of them? We want to ﬁll u p
this gap between maximum likelihood and minimax strategies and develop
a strategy that covers teh whole range of learning samples lengths including
zero length. However, this gap, and it is infact a gap, shows a theor etical
imperfectionofthecommonlyusedlearningprocedures, namely, of maximum
likelihood learning. The short sample problem in whole follows from the fa ct
that maximum likelihood learning as well as many other learning procedu res
have not been deduced from any explicit risk-oriented requirement to the
quality of post-learning recognition. We will formulate such risk-orie nted
requirements a priori and will see what type of learning procedures follow.
2 Basic deﬁnitions
Deﬁnition 1. An object is represented with a tuple
/angbracketleftbig
X,Y,Θ, pXY:X×Y×Θ→R/angbracketrightbig
4 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1
n= 1 n= 2
θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1
n= 5 n= 10
Figure 4: Example 2. Probability of a wrong decision (risk) for diﬀeren t
sizesnof the learning sample. The curve R(qML,θ) shows the risk of a
maximum likelihood strategy, R(qminmax,θ) is the risk of a minimax strategy,
min
qR(q,θ) is the minimal possible risk.
where
Xis a ﬁnite set of signal values x∈X;
Yis a ﬁnite set of states y∈Y;
Θis a ﬁnite set of models θ∈Θ;
pXY(x,y;θ)is a probability of a pair (x∈X,y∈Y)for a model θ∈Θ. A signal xis an observable parameter of recognized object whereas a
stateyis its hidden parameter. A pair ( x,y) is random and for each pair
(x∈X,y∈Y) its probability pXY(x,y;θ) exists. However, this probability
is not known because it depends on an unknown model θ. As for the model
θit is not random, it takes a ﬁxed but unknown value. Only the set Θ is
known that the value θbelongs to. Letzbe some random data that depend on a model θand take values
from a ﬁnite set Z. The data is speciﬁed with a tuple/angbracketleftbig
Z, pZ:Z×Θ→R/angbracketrightbig
wherepZ(z;θ) is a probability of data z∈Zfor model θ∈Θ.
5 Deﬁnition 2. A random data/angbracketleftbig
Z, pZ:Z×Θ→R/angbracketrightbig
that depends on a model
is called a learning data for an object/angbracketleftbig
X,Y,Θ, pXY:X×Y×Θ→R/angbracketrightbig
if
pXYZ(x,y,z;θ) =pXY(x,y;θ)·pZ(z;θ)for allx∈X,y∈Y,z∈Z,θ∈Θ. A learning sample (( xi,yi)|i= 1,2,...,n) used for supervised learning is
a special cases of learning data when
Z= (X×Y)nandpZ(z;θ) =n/productdisplay
i=1pXY(xi,yi;θ). A learning sample ( xi|i= 1,2,...,n) for unsupervised learning is another
special case of learning data when
Z=XnandpZ(z;θ) =n/productdisplay
i=1/summationdisplay
y∈YpXY(xi,y;θ). Any expert knowledge about the true model is also learning data. On e can
even consider the case when |Z|= 1 and therefore pZ(z;θ) = 1, which is
equivalent to the absence of any learning data at all. We do not restr ict the
learning data in any way except that for any ﬁxed model the learning data
zdepend neither on the current signal xnor on the current state yso that
pXYZ(x,y,z;θ) =pXY(x,y;θ)·pZ(z;θ) for allx∈X,y∈Y,z∈Z,θ∈Θ. Deﬁnition 3. A non-negative function q:X×Y×Z→Ris called a strategy
if/summationtext
y∈Yq(y|x,z) = 1for allx∈X,z∈Z. Valueq(y|x,z) of a strategy q:X×Y×Z→Ris a probability of a
randomized decision that the current state ofan object is y, given thecurrent
observed signal xand the available learning data z. The set of all strategies
q:X×Y×Z→Ris denoted Q. Letω:Y×Ybe a loss function whose value ω(y,y′) is the loss of a
decisiony′when the true state is y. Deﬁnition 4. RiskR(q,θ)of a strategy qon a model θis expected loss
R(q,θ) =/summationdisplay
z∈Z/summationdisplay
x∈X/summationdisplay
y∈YpXY(x,y;θ)pZ(z;θ)/summationdisplay
y′∈Yq(y′|x,z)ω(y,y′).
6 Let us be reminded that throughout the paper the sets X,Y,Zand Θ
are assumed to be ﬁnite. This allows a much more transparent formu lation
of main results. Allowing some of the sets to be inﬁnite would require ﬁn er
mathematical tools and the results might be obscured by unnecess ary tech-
nical details.
3 Improper and Bayesian strategies. One can see that the risk of a strategy depends not only on the str ategy itself
but also on the model that the strategy is applied to. Therefore, in a general
case it is not possible to prefer some strategy q1to another strategy q2. The
risk ofq1may be better than the risk of q2on some models and worse on the
others. However, it is possible to prefer strategy q2to strategy q1if the risk
ofq1is greater than the risk of q2on all models. In this case we will say that
q2dominates q1andq1is dominated by q2. Deﬁnition 5. A strategy q0is called improper if a strategy q∗exists such
thatR(q0,θ)> R(q∗,θ)for allθ∈Θ. We want to exclude all improper from consideration strategies and d erive
a common form of all the rest. Let Tdenote the set of all non-negative
functions τ: Θ→Rsuch that/summationtext
θ∈Θτ(θ) = 1. Functions of such type will be
reﬀerred to as weight functions. Deﬁnition 6. A strategy q∗is called Bayesian if there exists a weight func-
tionτ∈Tsuch that
q∗= argmin
q∈Q/summationdisplay
θ∈Θτ(θ)R(q,θ). Theorem 1. Each strategy q0∈Qis either Bayesian or improper, but never
both. Proof. For a given strategy q0let us deﬁne a function F:T×Q→R,
F(τ,q) =/summationdisplay
θ∈Θτ(θ)/bracketleftbig
R(q,θ)−R(q0,θ)/bracketrightbig
.
7 According to Deﬁnition 4, for any ﬁxed θthe riskR(q,θ) is a linear function
of probabilities q(y|x,z). Consequently, for any ﬁxed τthe function Fis also
a linear function of probabilities q(y|x,z). Similarly, function Fis a linear
function of weights τ(θ) for any ﬁxed strategy q. The set Qof strategies
and the set Tof weight functions are both closed convex sets. Consequently,
due to the known duality theorem [1, 2, 4] function Fhas a saddle point
(τ∗∈T,q∗∈Q) such that
max
τ∈Tmin
q∈QF(τ,q) =F(τ∗,q∗) = min
q∈Qmax
τ∈TF(τ,q),
where
q∗= argmin
q∈Qmax
τ∈TF(τ,q), τ∗= argmax
τ∈Tmin
q∈QF(τ,q). It is obvious that F(τ,q0) = 0 for any τ∈T. Therefore, the inequality
min
q∈QF(τ,q)≤0 holds for every τ∈Tand, consequently,
max
τ∈Tmin
q∈QF(τ,q) =F(τ∗,q∗)≤0. Therefore, there are two mutually exclusive cases: either F(τ∗,q∗)<0 or
F(τ∗,q∗) = 0. In such way the proof of the theorem is reduced to proving
the following four propositions:
Proposition 1. If the strategy q0is Bayessian then F(τ∗,q∗) = 0. Proposition 2. If F(τ∗,q∗) = 0 then the strategy q0is Bayessian. Proposition 3. If the strategy q0is improper then F(τ∗,q∗)<0. Proposition 4. If F(τ∗,q∗)<0 then the strategy q0is improper. Proof of Proposition 1. If the strategy q0is Bayessian then according to
Deﬁnition 6 a weight function τ0exists such that inequality
/summationdisplay
θ∈Θτ0(θ)R(q,θ)≥/summationdisplay
θ∈Θτ0(θ)R(q0,θ)
is valid for all q∈Q. Consequently, for all q∈Qthe chain
0≤/summationdisplay
θ∈Θτ0(θ)[R(q,θ)−R(q0,θ)] =F(τ0,q)≤max
τ∈TF(τ,q)
8 is also valid. Since all numbers max τ∈TF(τ,q),q∈Q, are not negative the
least of them is also not negative and
min
q∈Qmax
τ∈TF(τ,q) =F(τ∗,q∗)≥0
Fromthisinequality itfollowsthat F(τ∗,q∗) = 0because acase F(τ∗,q∗)>0
is impossible. Proof of Proposition 2. Let F(τ∗,q∗) = 0 then
0 =F(τ∗,q∗) = max
τ∈Tmin
q∈QF(τ,q) = min
q∈QF(τ∗,q) =
= min
q∈Q/summationdisplay
θ∈Θτ∗(θ)/bracketleftbig
R(q,θ)−R(q0,θ)/bracketrightbig
=
= min
q∈Q/bracketleftbigg/summationdisplay
θ∈Θτ∗(θ)R(q,θ)/bracketrightbigg
−/summationdisplay
θ∈Θτ∗(θ)R(q0,θ). It implies the equality
min
q∈Q/summationdisplay
θ∈Θτ∗(θ)R(q,θ) =/summationdisplay
θ∈Θτ∗(θ)R(q0,θ)
and therefore,
q0= argmin
q∈Q/summationdisplay
θ∈Θτ∗(θ)R(q,θ),
which means that q0is Bayesian according to Deﬁnition 6. Proof of Proposition 3. If the strategy q0is improper then according to
Deﬁnition 5 a strategy q1exists such that inequality R(q1,θ)< R(q0,θ)
holds for all θ. The set of models is ﬁnite and therefore, a value ε <0 exists
such that for any θinequality R(q1,θ)−R(q0,θ)≤εholds and a chain
0> ε≥/summationdisplay
θ∈Θτ(θ)[R(q1,θ)−R(q0,θ)] =F(τ,q1)≥min
q∈QF(τ,q)
is valid for any τ∈T. Since all numbers min q∈QF(τ,q),τ∈T, are not
greater then εthe greatest of them is also not greater then εand
max
τ∈Tmin
q∈QF(τ,q) =F(τ∗,q∗)≤ε <0.
9 Proof of Proposition 4. Let F(τ∗,q∗)<0 then
F(τ∗,q∗) = min
q∈Qmax
τ∈TF(τ,q) = max
τ∈TF(τ,q∗) =
= max
τ∈T/summationdisplay
θ∈Θτ(θ)/bracketleftbig
R(q∗,θ)−R(q0,θ)/bracketrightbig
= max
θ∈Θ/bracketleftbig
R(q∗,θ)−R(q0,θ)/bracketrightbig
and therefore
max
θ∈Θ/bracketleftbig
R(q∗,θ)−R(q0,θ)/bracketrightbig
<0. Consequently, the inequality R(q∗,θ)< R(q0,θ) holds for all models θ∈Θ
andq0is improper according to Deﬁnition 5. The theorem gives good reasons to reappraise lot of well-known met hods
that are commonly used as something self-evident. Let us illustrate this
criticism with two simple examples. The ﬁrst example considers a certa in
method of recognition without learning and the second relates to ma ximum
likelihood learning. In both examples the loss function is
ω(y,y′) =/braceleftBigg
0,ify=y′,
1,ify/ne}ationslash=y′. Example 3. Letxbe an image of a letter, ybe its name and θbe a position
of the letter in a ﬁeld of vision. Let the function pXY:X×Y×Θ→Rbe
constructively deﬁned so that probability pXY(x,y;θ) can be calculated for
each triple x,y,θ. In this case when an image xwith an unknown position
θis observed the decision y∗(x) about the name of the letter has to be of the
form
y∗(x) = argmax
y∈Y/summationdisplay
θ∈Θτ(θ)pXY(x,y;θ). (1)
Theorem 1 reveals a certain weakness of the commonly used form
argmax
y∈Ymax
θ∈ΘpXY(x,y;θ). (2)
The strategy (2) could be represented in the form (1) if the weight sτ(θ) in
(1) could be chosen individually for each observation x∈X. However, each
10 Bayessian strategy is speciﬁed with its own weight function τ: Θ→Rso
that weights are assigned to elements of the set Θ, not of the set Θ ×X. As a rule, the strategy (2) cannot be represented in the form (1) with ﬁxed
weightsτ(θ) that do not depend on x. It means that the strategy (2) is not
Bayessian and is dominated by some other strategy that for each p osition of
the letter recognizes its name better then strategy (2). Example 4. Let the sets X,Yand Θ be speciﬁed for the recognized object
as well as a function pXY:X×Y×Θ→R. Let the learning information
be a random learning sample z= ((xi,yi)|i= 1,2,...,n) such that
pZ(z;θ) =n/productdisplay
i=1pXY(xi,yi;θ). Then the decision y∗about the current state y0based on the current signal
x0and available learning sample zhas to be of the form
y∗= argmax
y0∈Y/summationdisplay
θ∈Θτ(θ)n/productdisplay
i=0p(xi,yi;θ) (3)
for some ﬁxed τthat does not depend on z. One can see that the commonly
used maximum likelihood strategy
y∗= argmax
y0p(x0,y0;θML(z)), (4)
θML(z) = argmax
θ∈Θn/productdisplay
i=1p(xi,yi;θ)
can almost never be represented in the form (3) with constant weig hts and
therefore is not Bayessian. It means that some other strategy e xists that
makes a decision about the current state based both on current s ignal and
learning information and for each model makes it better than strat egy (4).
4 A gap between maximum likelihood and
minimax strategies. We consider maximum likelihood and minimax strategies and specify a gap
between them.
11 Let us deﬁne for each θ∈Θ a strategy qopt(θ) = argminq∈QR(q,θ) that
assigns a probability qopt(y|x,z;θ) for each triple ( x,y,z). The strategy
qopt(θ) is the best possible strategy that should be used if a true model we re
known. Since the model is known no learning data are needed. For an y ﬁxed
modelθa strategy q(θ) :X×Y×Z→Rcan be replaced with a strategy
qX(θ) :X×Y→Rwith the same risk. Probabilities q(y|x,z;θ) have to be
transformed into probabilities qX(y|x;θ) according to expression
qX(y|x;θ) =/summationdisplay
z∈ZpZ(z;θ)q(y|x,z;θ)
and so the chain
R(q,θ) =/summationdisplay
z∈Z/summationdisplay
x∈X/summationdisplay
y∈YpXY(x,y;θ)pZ(z;θ)/summationdisplay
y′∈Yq(y′|x,z;θ)ω(y,y′) =
=/summationdisplay
x∈X/summationdisplay
y∈YpXY(x,y;θ)/summationdisplay
y′∈Yω(y,y′)/summationdisplay
z∈ZpZ(z;θ)q(y′|x,z;θ) =
=/summationdisplay
x∈X/summationdisplay
y∈YpXY(x,y;θ)/summationdisplay
y′∈YqX(y′|x;θ)ω(y,y′) =R(qX,θ).
is valid for each model θ. Consequently, for each θthe equality
min
q∈QR(q,θ) = min
qX∈QXR(qX,θ) (5)
is valid. The symbol QXin (5) designates a set of all strategies of the form
qX:X×Y→Rthat do not use the learning data. Deﬁnition 7. A strategy qML:X×Y×Z→Ris called a maximum
likelihood strategy if for each triple (x,y,z)it speciﬁes a probability
qML(y|x,z) =qopt
X(x|y;θML(z)),
whereqopt
X(θ) = argmin
qX∈QXR(qX,θ)andθML(z) = argmax
θ∈ΘpZ(z;θ). In other words, maximum likelihood strategies use the learning data zto
estimate a model θand make a decision that minimizes the expected loss
with an assumption that the estimated model is the true model.
12 AsithasbeenquotedforExamples3and4,asarule, maximumlikelihood
strategies cannot be represented in a form of a Bayessian strate gy
qB= argmin
q∈Q/summationdisplay
θ∈Θτ(θ)R(q,θ)
with ﬁxed weights τ(θ) that do not depend on the learning data. In such
cases the maximum likelihood strategy qMLmay be dominated with another
strategy of the form X×Y×Z→R. Minimax strategies are free of this
ﬂaw. Deﬁnition 8. Strategyargmin
q∈Qmax
θ∈ΘR(q,θ)is called a minimax strategy. Theorem 2. No minimax strategy is improper. Proof. Let us prove an equivalent statement that any improper strategy q0is
not minimax. Indeed, as far as q0is improper another strategy q1exists such
thatR(q1,θ)< R(q0,θ) for allθ. Therefore, max θR(q1,θ)<maxθR(q0,θ)
and min qmaxθR(q,θ)<maxθR(q0,θ) andq0is not argminqmaxθR(q0,θ). Though maximum likelihood strategy may be improper whereas minimax
strategy is never improper the ﬁrst one has an essential advanta ge over the
second. There is a rather wide class of learning data such that the m aximum
likelihood strategy is in a sense consistent for any recognized objec t whereas
there isa rather wide class of recognized objects such thatthe min imax strat-
egy is not consistent for any learning data. Let us exactly formulat e these
statements and prove them. Letz∈Zbe a random variable that depends on model θand let for each
z∈Zandθ∈Θ a probabillity pZ(z;θ) be given. We will say that this
dependence is essential if for each two diﬀerent models θ1/ne}ationslash=θ2a valuez∗
exists such that pZ(z∗;θ1)/ne}ationslash=pZ(z∗;θ2). Letzn= (zi|i= 1,2,...,n)∈Znbe
a learning sample, pZn(zn;θ∗) =/producttextn
i=1pZ(zi;θ∗) be a probability of a sample
andθML(zn) = argmaxθpZn(zn;θ) be a maximum likelihood estimation of
the model. Consistency is a generally known property of maximum likelihood esti-
mate. In the considered case this property may be formulated in a s imple
13 way that the probability of inequality θML(zn)/ne}ationslash=θ∗converges to zero when
nincreases or, formally,
lim
n→∞/summationdisplay
zn∈Znerrn/productdisplay
i=1pZ(zi;θ∗) = 0 (6)
where
Zn
err={zn∈Zn|θML(zn)/ne}ationslash=θ∗}. (7)
The consistency of a maximum likelihood estimations is a base for a proo f of
the following theorem about consistency of maximum likelihood strate gy. Theorem 3. Letzbe random variable that takes values from a set Zac-
cording to probability distribution pZ(z;θ)that essentially depends on θ;
letnbe a positive integer and zn= (zi|i= 1,2,...,n)∈Znbe a random
learning sample with probability distribution pZn(zn;θ) =/producttextn
i=1pZ(zi;θ);
letqML
n:X×Y×Zn→Rbe a maximum likelihood strategy for an object
/an}bracketle{tX,Y,Θ,pXY:X×Y×Θ→R/an}bracketri}htand learning data /an}bracketle{tZn,pZn:Zn×Θ→R/an}bracketri}ht. Then
lim
n→∞max
θ∈Θ/bracketleftbig
R(qML
n,θ)−min
q∈QR(q,θ)/bracketrightbig
= 0. Proof. As far as a set Θ is ﬁnite the proof of the theorem is reduced to proo f
of the equality
lim
n→∞/bracketleftbig
R(qML
n,θ)−min
q∈QR(q,θ)/bracketrightbig
= 0 (8)
for anyθ. The subsequent proof is based on equality (5), on equalities (6)
and (7) that express consistency of maximum likelihood estimates an d on
equality
R(qML
n,θ) =/summationdisplay
zn∈ZnpZn(zn;θ) min
qX∈QXR(qX,θML(zn)),
whereθML(zn) = argmax
θ∈ΘpZn(zn;θ),
14 that follows from Deﬁnition 7. The following chain is valid:
lim
n→∞[R(qML
n,θ)−min
q∈QR(q,θ)] = lim
n→∞[R(qML
n,θ)−min
qX∈QXR(qX,θ)] =
= lim
n→∞[/summationdisplay
zn∈ZnpZn(zn;θ) min
qX∈QXR(qX,θML(zn))−min
qX∈QXR(qX,θ)]
= lim
n→∞/summationdisplay
zn∈ZnpZn(zn;θ)[ min
qX∈QXR(qX,θML(zn))−min
qX∈QXR(qX,θ)]
= lim
n→∞/summationdisplay
zn∈ZnerrpZn(zn;θ)[ min
qX∈QXR(qX,θML(zn))−min
qX∈QXRX(qX,θ)]
≤lim
n→∞/summationdisplay
zn∈ZnerrpZn(zn;θ)[max
y∈Ymax
y′∈Yw(y,y′)−min
y∈Ymin
y′∈Yw(y,y′)]
= lim
n→∞{[max
y∈Ymax
y′∈Yw(y,y′)−min
y∈Ymin
y′∈Yw(y,y′)]/summationdisplay
zn∈ZnerrpZn(zn;θ)}
= [max
y∈Ymax
y′∈Yw(y,y′)−min
y∈Ymin
y′∈Yw(y,y′)] lim
n→∞/summationdisplay
zn∈ZnerrpZn(zn;θ) = 0. It follows from a chain that for any θan inequality
lim
n→∞/bracketleftbig
R(qML
n,θ)−min
q∈QR(q,θ)/bracketrightbig
≤0
holds. The diﬀerence R(qML
n,θ)−minq∈QR(q,θ) is never negative and so (8)
is proved. So, withthe increasing length oflearning sample therisk function of m ax-
imum likelihood strategy becomes arbitrarily close to the minimum possib le
risk function. Minimax strategy has not this property. Moreover, for certain
class of objects minimax strategies simply ignore the learning sample, no
matter how long it is. Theorem4. Letfor anobject /an}bracketle{tX,Y,Θ,pXY:X×Y×Θ→R/an}bracketri}hta pair(θ∗,q∗
X)
exists such that
q∗
X= argmin
qX∈QXR(qX,θ∗), θ∗= argmax
θ∈ΘR(q∗
X,θ). Then the inequality
max
θ∈ΘR(q,θ)≥max
θ∈ΘR(q∗
X,θ) (9)
is valid for any learning data /an}bracketle{tZ,pZ:Z×Θ→R/an}bracketri}htand any strategy q:X×
Y×Z→R.
15 Proof. For any strategy q∈Qwe have the chain
max
θ∈ΘR(q,θ)≥R(q,θ∗)≥min
q∈QR(q,θ∗) =
= min
qX∈QXR(qX,θ∗) =R(q∗
X,θ∗) = max
θ∈ΘR(q∗
X,θ). The theorem shows that for some objects theminimax approach is p artic-
ularly inappropriate because it enforces to ignore any learning data . There
is nothing unusual in conditions of the Theorem 4. Examples 1 and 2 in
Introduction show just the cases when these conditions are satis ﬁed. So, thereisafollowinggapbetweenmaximum likelihoodandminimaxstrat e-
gies. Maximum likelihood strategy may be dominated with other strate gy. In this case it can be improved and, consequently, it is not optimal fr om any
point of view. However, for wide class of learning data maximum likelihoo d
strategies are consistent and so their chortage does not become apparent
when learning sample of an arbitrary size may be obtained. Cases of le arn-
ing samples of ﬁxed sizes, especially, short samples form an area of im proper
application of maximum likelihood strategies. This area is not covered w ith
minimaxstrategies. Thoughminimaxstrategiesaredominatedwithno strat-
egy, for rather wide class of objects minimax requirement enforce s to ignore
any learning sample, no matter how long it is.
5 Minimax deviation strategies. This section is aimed at developing a Bayesian consistent strategy th at has
to ﬁll a gap between maximum likelihood and minimax strategies. Deﬁnition 9. A strategy argmin
q∈Qmax
θ∈Θ/bracketleftbig
R(q,θ)−min
q′∈QR(q′,θ)/bracketrightbig
is called mini-
max deviation strategy. Minimax deviation strategies do not have the drawback of the minimax
strategies. A theorem that is similar to Theorem 3 for maximum likelihoo d
strategies is also valid for minimax deviation strategies.
16 Theorem 5. Letzbe random variable that takes values from a set Zac-
cording to probability distribution pZ(z;θ)that essentially depends on θ;
letnbe a positive integer and zn= (zi|i= 1,2,...,n)∈Znis a random
learning sample with probability distribution pZn(zn;θ) =/producttextn
i=1pZ(zi;θ);
letq∗
n:X×Y×Zn→Rbe a minimax deviation strategy for an object
/an}bracketle{tX,Y,Θ,pXY:X×Y×Θ→R/an}bracketri}htand learning data /an}bracketle{tZn,pZn:Zn×Θ→R/an}bracketri}ht. Then
lim
n→∞max
θ∈Θ/bracketleftbig
R(q∗
n,θ)−min
q∈QR(q,θ)/bracketrightbig
= 0. (10)
Proof. The Theorem is a straighforward consequence of Deﬁnition 9 and
the Theorem 3. Let qML
nbe a maximum likelihood strategy for an object
/an}bracketle{tX,Y,Θ,pXY:X×Y×Θ→R/an}bracketri}htand learning data /an}bracketle{tZn,pZn:Zn×Θ→R/an}bracketri}ht. It follows from Deﬁnition 9 that
max
θ∈Θ/bracketleftbig
R(q∗
n,θ)−min
q∈QR(q,θ)/bracketrightbig
≤max
θ∈Θ/bracketleftbig
R(qML
n,θ)−min
q∈QR(q,θ)/bracketrightbig
for anyn. It follows from Theorem 3 that
lim
n→∞max
θ∈Θ/bracketleftbig
R(q∗
n,θ)−min
q∈QR(q,θ)/bracketrightbig
≤
≤lim
n→∞max
θ∈Θ/bracketleftbig
R(qML
n,θ)−min
q∈QR(q,θ)/bracketrightbig
= 0. As far as the diﬀerence [ R(q∗
n,θ)−minq∈QR(q,θ)/bracketrightbig
is negative for no model
the equality (10) is proved. Let usnotethat theproofoftheTheorem 10shows notonly acons istency
of minimax deviation strategy. It shows also that minimax deviation st rat-
egy converges to desired result not slower than maximum likelihood st rategy. Similarly, one can show that this advantage of minimax deviation strat egy
holds as compared with any consistent strategy and from this point of view
it is the best of all consistent strategies. Following theorem states that minimax deviation strategies are also in ap-
propriate for recognition of certain type of objects.
17 Theorem 6. Let for an object /an}bracketle{tX,Y,Θ,p:X×Y×Θ→R/an}bracketri}hta modelθ∗and
a strategy q∗
Xexist such that
q∗
X= argmin
qX∈QX[RX(qX,θ∗)−min
q′
X∈QXRX(q′
X,θ∗)], (11)
θ∗= argmax
θ∈Θ[RX(q∗
X,θ)−min
q′
X∈QXRX(q′
X,θ)]. (12)
Then the inequality
max
θ∈Θ[R(q,θ)−min
qX∈QXR(qX,θ)]≥max
θ∈Θ[R(q∗
X,θ)−min
qX∈QXR(qX,θ)]
holds for any learning data /an}bracketle{tZ,pZ:Z×Θ→R/an}bracketri}htand any strategy q∈Q. Proof. In fact, proof of the theorem does not diﬀer from the proof of th e
Theorem 4. However, the consequences of this theorem for minimax deviation s trate-
gies are not so destructive as those of Theorem 4 for minimax strat egies. In
fact, conditions (11) and (12) imply that a strategy q∗
X∈QXexists that
does not use learning information and assures minimal possible risk fo r each
model,
R(q∗
X,θ) = min
qX∈QXR(qX,θ) for allθ∈Θ. In this case, any learning data are needless and has to be omitted by any
strategy. Evidently, minimax deviation strategy is not improper and, conseque ntly,
is Bayessian. The following theorem shows how the corresponding we ight
function has to be obtained. Theorem 7. Minimax deviation strategy
q∗= argmin
q∈Qmax
θ∈Θ/bracketleftbig
R(q,θ)−min
qX∈QXR(qX,θ)/bracketrightbig
is a Bayesian strategy argmin
q∈Q/summationtext
θ∈Θτ∗(θ)R(q,θ)with respect to weight function
τ∗= argmax
τ∈T/bracketleftBigg
min
q∈Q/summationdisplay
θ∈Θτ(θ)R(q,θ)−/summationdisplay
θ∈Θτ(θ) min
qX∈QXR(qX,θ)/bracketrightBigg
.(13)
18 Proof. Let us deﬁne a function F:T×Q→R,
F(τ,q) =/summationdisplay
θ∈Θτ(θ)R(q,θ)−/summationdisplay
θ∈Θτ(θ) min
qX∈QXR(qX,θ)
and express q∗andτ∗in terms of F,
q∗= argmin
q∈Qmax
θ∈Θ/bracketleftbig
R(q,θ)−min
qX∈QXR(qX,θ)/bracketrightbig
= argmin
q∈Qmax
τ∈T/summationdisplay
θ∈Θτ(θ)/bracketleftbig
R(q,θ)−min
qX∈QXR(qX,θ)/bracketrightbig
= argmin
q∈Qmax
τ∈TF(τ,q),
τ∗= argmax
τ∈Tmin
q∈QF(τ,q). The function Fis a linear function of qfor ﬁxed τand a linear function of
τfor ﬁxed qand is deﬁned on a Cartesian product of two closed convex sets
TandQ. In such case a pair ( τ∗,q∗) is a saddle point [1, 2, 4],
min
q∈Qmax
τ∈TF(τ,q) =F(τ∗,q∗) = max
τ∈Tmin
q∈QF(τ,q),
that implies F(τ∗,q∗) = min
q∈QF(τ∗,q) and
q∗= argmin
q∈QF(τ∗,q) =
= argmin
q∈Q/bracketleftBigg/summationdisplay
θ∈Θτ∗(θ)R(q,θ)−/summationdisplay
θ∈Θτ∗(θ) min
qX∈QXR(qX,θ)/bracketrightBigg
=
= argmin
q∈Q/summationdisplay
θ∈Θτ∗(θ)R(q,θ). Insuch way developing minimax deviationstrategyisreduced to calcu lat-
ing weights τ(θ) ofmodels that maximize concave function (13). In described
below experiments general purpose methods of non-smooth optim ization [6]
were used.
19 6 Experiments
Minimax deviation strategies have been built for objects considered in In-
troduction in Examples 1 and 2. Minimax deviation strategies have bee n
compared with maximum likelihood and minimax strategies. Results are
presented on Figures 5 and 6 that show risk R(q,θ) of the strategies as a
function of a model for several learning sample sizes. Figure 5 relat es to
Example 1 and Figure 6 to Example 2.
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
n= 1 n= 2
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
θ/Bullet
−6−3−0 3 6/Bullet /Bullet /Bullet /BulletR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
n= 3 n= 10
Figure 5: Example 1. Probability of making a wrong decision for diﬀeren t
sizesnof the learning sample. The dashed line shows the risk of a minimax
deviation strategy. The curve R(qML,θ) is the risk of a maximum likelihood
strategy. The curve R(qminmax,θ) is the risk of a minimax strategy. The
curve min
qR(q,θ) is the minimum possible risk for each model.
7 Conclusion
The paper analyzes the problem when for given object
/angbracketleftbig
X,Y,Θ, pXY:X×Y×Θ→R/angbracketrightbig
,
loss function w:Y×Y→R, learning data source/angbracketleftbig
Z, pZ:Z×Θ→R/angbracketrightbig
,
observed current signal xand available learning data za decision y∗about
20 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1
n= 1 n= 2
θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1 θR(q,θ)
min
qR(q,θ)R(qminmax,θ)R(qML,θ)
/Bullet0/Bullet0.5/Bullet1
n= 5 n= 10
Figure 6: Example 2. Probability of making a wrong decision for diﬀeren t
sizesnof the learning sample. The dashed line shows the risk of a minimax
deviation strategy. The curve R(qML,θ) is the risk of a maximum likelihood
strategy. The curve R(qminmax,θ) is the risk of a minimax strategy. The
curve min
qR(q,θ) is the minimum possible risk for each model.
current hidden state yhas to be made. The wide class of commonly used
strategies make the decision of a form
y∗= argmin
y′∈Y/summationdisplay
y∈YpXY(x,y;θest(z))w(y,y′) (14)
whereθest:Z→Θ is a reasonable estimating a model θbased on learning
dataz. It meansthatthelearning dataareused tochooseasinglebest mo del
and the objects are recognized as if this best model equals the tru e model. The approach is acceptable if learning data are arbitrarily long learnin g sam-
ples and estimator θest:Z→Θ is consistent. If the learning information has
a ﬁxed format, for example, is a learning sample of limited size then the ap-
proach gives no guarantee for subsequent recognition. Indeed, the approach
is not deduced from any risk-oriented requirement. Reasonable re quirement
21 to the quality of post-learning recognition implies the decision of the f orm
y∗= argmin
y′∈Y/summationdisplay
θ∈Θτ(θ)pZ(z;θ)/summationdisplay
y∈YpXY(x,y;θ)w(y,y′) (15)
that diﬀers from (14). Moreover, any decision that diﬀers from (1 5) can be
replaced with a decision of the form (15) with the better recognition quality. Thereisnothingindecision(15)thatcouldbetreatedasaselectings omebest
modelofthemodelsetandsonoquestionstandswhatestimator θest:Z→Θ
has to be used. No model has to be selected, on the contrary, all m odels have
to take part in decision with their weights. It is essential that the we ights do
not depend onlearning data, they aredetermined by requirement t o searched
strategyforconcreteappliedsituation. Thepapershowsawayfo rcomputing
these weights for minimax deviation strategy that is appropriate fo r learning
samples of any length and in such way ﬁlls a gap between maximum likeli-
hood and minimax startegies. Minimax deviation strategy is not at all a single strategy that is reaso n-
able in such or other application. Many other strategies are approp riate too,
for example, a strategies of the form
argmin
q∈Qmax
θ∈ΘR(q,θ)−α(θ)
β(θ)(16)
with predeﬁned numbers α(θ) andβ(θ)>0. Minimax strategy is a special
case of (16) when α(θ) = 0,β(θ) = 1, minimax deviation strategy is a case
whenα(θ) = min q∈QR(q,θ),β(θ) = 1. A reasonable modiﬁcation of mini-
max deviation strategy is a case when α(θ) = 0,β(θ) = min q∈QR(q,θ). The
numbers α(θ) may be risks of some already developed strategy and this is a
case when the developer wants to check whether the better stra tegy is possi-
ble. At last, numbers α(θ) may be simply desired values of risks in concrete
applied situation. Requirements of the form (16) together with various loss function s deter-
mine various applied situations and obtained results show the way to c ope
with all them. It has become quite clear now that each strategy of t he form
(16) may be represented in the form (15) because, obviously, no o f them is
improper. Obtained results imply unexpected conclusion that learnin g data
22 take part in a decision (15) in a uniﬁed form that depends neither on a pplied
situation nor on recognized object. So, no question stands more h ow the
learning data have to inﬂuence the decision about current state wh en the
current signal is observed. Learning data inﬂuence the decision via and only
via probabilities pZ;(z;θ), not via choise of some best model of the model
set. References
[1] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimiza-
tion. Springer Verlag, 2000.
[2] S. Boyd and L. Vandenberghe. Convex Optimization . Cambrige Univer-
sity Press, 2004.
[3] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classiﬁ-
cation. Wiley, 2000.
[4] J.-B. Hiriart-Urruty and C. Lemarechal. Fundamentals of Convex Anal-
ysis. Springer Verlag, 2002.
[5] Herbert Robbins. Asymptotically Subminimax Solutions of Compoun d
StatisticalDecisionProblems. InJerzyNeyman, editor, Proceedingsof the
Second Berkeley Symposium on Mathematical Statistics and P robability ,
pages 131–148. University of California Press, 1951.
[6] N. Z. Shor. Nondiﬀerentiable Optimization and Polynomial Problems . Nonconvex Optimization and Its Applications. Springer, 1998.
[7] Andrew R. Webb. Statistical Pattern Recognition . Wiley, 2002.
23