{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fae9dbb"
      },
      "source": [
        "# TF-IDF Tutorial\n",
        "\n",
        "This notebook explains the concept of TF-IDF (Term Frequency-Inverse Document Frequency) and shows how to calculate it both manually and using the `scikit-learn` library.\n",
        "\n",
        "TF-IDF is a widely used technique in natural language processing (NLP) and information retrieval to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "**Why is TF-IDF useful?**\n",
        "\n",
        "When we represent text as a Bag of Words, all words are treated equally. However, some words are more important than others for understanding the content of a document. For example, common words like \"the\", \"a\", or \"is\" appear very frequently but don't tell us much about the specific topic of a document. On the other hand, words that are unique to a particular document or a small set of documents are likely more important.\n",
        "\n",
        "TF-IDF helps to address this by giving higher scores to words that are frequent in a document but rare in the overall collection of documents.\n",
        "\n",
        "TF-IDF is calculated by multiplying two components:\n",
        "\n",
        "1.  **Term Frequency (TF)**: How often a word appears in a single document.\n",
        "2.  **Inverse Document Frequency (IDF)**: How rare or common a word is across the entire collection of documents.\n",
        "\n",
        "Let's explore each of these components and then see how they are combined to calculate TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "274d98f5"
      },
      "source": [
        "### Term Frequency (TF)\n",
        "\n",
        "**Term Frequency (TF)** measures how frequently a term (word) appears in a document. Since a longer document might have a word appear more times than a shorter document, even if the word is not more important, TF is often normalized. A common way to normalize is to divide the number of times a word appears in a document by the total number of words in that document.\n",
        "\n",
        "The formula for Term Frequency (TF) is:\n",
        "\n",
        "$$ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
        "\n",
        "Let's look at the Python code provided to calculate Term Frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3ddbd42",
        "outputId": "0afce98d-dd16-427d-f8c6-b2b15c31dfca"
      },
      "source": [
        "import math\n",
        "\n",
        "def compute_tf(word, document):\n",
        "    \"\"\"\n",
        "    Computes the Term Frequency (TF) for a given word in a document.\n",
        "\n",
        "    Args:\n",
        "        word (str): The word to compute TF for.\n",
        "        document (list): A list of words representing the document.\n",
        "\n",
        "    Returns:\n",
        "        float: The TF value for the word in the document.\n",
        "    \"\"\"\n",
        "    return document.count(word) / len(document)\n",
        "\n",
        "# Example usage (from the provided code):\n",
        "doc1 = \"I love NLP NLP\".split()\n",
        "doc2 = \"NLP loves Python\".split()\n",
        "all_docs = [doc1, doc2]\n",
        "\n",
        "print(f\"TF for 'NLP' in doc1: {compute_tf('NLP', doc1):.3f}\")\n",
        "print(f\"TF for 'love' in doc2: {compute_tf('loves', doc2):.3f}\") # Note: case sensitive without cleaning"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF for 'NLP' in doc1: 0.500\n",
            "TF for 'love' in doc2: 0.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYBCJ_uGPFoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "251defeb"
      },
      "source": [
        "### Inverse Document Frequency (IDF)\n",
        "\n",
        "**Inverse Document Frequency (IDF)** measures how important a word is across the entire collection of documents (corpus). It downweights words that appear frequently in many documents and gives higher scores to words that are rare.\n",
        "\n",
        "The idea is that words that appear in almost every document are likely not very informative for distinguishing between documents. Words that appear in only a few documents are more likely to be specific to those documents and therefore more important.\n",
        "\n",
        "The formula for Inverse Document Frequency (IDF) is:\n",
        "\n",
        "$$ IDF(t, D) = \\log\\left(\\frac{\\text{Total number of documents } |D|}{\\text{Number of documents containing term } t}\\right) $$\n",
        "\n",
        "Where:\n",
        "*   $|D|$ is the total number of documents in the corpus.\n",
        "*   Number of documents containing term $t$ is the number of documents where the term $t$ appears at least once.\n",
        "\n",
        "The logarithm is used to dampen the effect of very large numbers of documents.\n",
        "\n",
        "Let's look at the Python code provided to calculate Inverse Document Frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b78834d",
        "outputId": "41961452-e8fa-4786-badb-ae9e2cded355"
      },
      "source": [
        "def compute_idf(word, documents):\n",
        "    \"\"\"\n",
        "    Computes the Inverse Document Frequency (IDF) for a given word in a collection of documents.\n",
        "\n",
        "    Args:\n",
        "        word (str): The word to compute IDF for.\n",
        "        documents (list of list): A list of documents, where each document is a list of words.\n",
        "\n",
        "    Returns:\n",
        "        float: The IDF value for the word across the documents.\n",
        "    \"\"\"\n",
        "    # Count the number of documents containing the word\n",
        "    num_docs_containing_word = sum(1 for doc in documents if word in doc)\n",
        "\n",
        "    # Add 1 to the denominator to avoid division by zero if a word is not in any document\n",
        "    # This is a common practice, sometimes called \"smooth IDF\"\n",
        "    # The formula used here is simpler and matches the one provided in the prompt.\n",
        "    # If num_docs_containing_word is 0, this will raise a ZeroDivisionError.\n",
        "    # In a real-world scenario, you might handle this case (e.g., return 0 or a very large number).\n",
        "    # Based on the provided code, we assume the word will be in at least one document for calculation.\n",
        "\n",
        "    return math.log(len(documents) / num_docs_containing_word)\n",
        "\n",
        "# Example usage (using the same documents as before):\n",
        "# doc1 = \"I love NLP NLP\".split()\n",
        "# doc2 = \"NLP loves Python\".split()\n",
        "# all_docs = [doc1, doc2] # Already defined in the previous cell\n",
        "\n",
        "print(f\"IDF for 'NLP': {compute_idf('NLP', all_docs):.3f}\") # Appears in both docs\n",
        "print(f\"IDF for 'love': {compute_idf('love', all_docs):.3f}\") # Appears only in doc1\n",
        "print(f\"IDF for 'Python': {compute_idf('Python', all_docs):.3f}\") # Appears only in doc2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF for 'NLP': 0.000\n",
            "IDF for 'love': 0.693\n",
            "IDF for 'Python': 0.693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bc9eea4"
      },
      "source": [
        "### TF-IDF Calculation\n",
        "\n",
        "The **TF-IDF** score for a word in a document is simply the product of its Term Frequency (TF) and its Inverse Document Frequency (IDF).\n",
        "\n",
        "The formula for TF-IDF is:\n",
        "\n",
        "$$ TF-IDF(t, d, D) = TF(t, d) \\times IDF(t, D) $$\n",
        "\n",
        "Where:\n",
        "*   $TF(t, d)$ is the Term Frequency of term $t$ in document $d$.\n",
        "*   $IDF(t, D)$ is the Inverse Document Frequency of term $t$ across the collection of documents $D$.\n",
        "\n",
        "A high TF-IDF score means that the word is frequent in the specific document but rare in the overall collection of documents. This suggests that the word is likely important and relevant to the topic of that document.\n",
        "\n",
        "Let's look at the Python code provided to calculate TF-IDF using the `compute_tf` and `compute_idf` functions we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702ef428",
        "outputId": "3a7d868b-98f5-4505-c94a-e8142c4a4842"
      },
      "source": [
        "def compute_tf_idf(word, document, documents):\n",
        "    \"\"\"\n",
        "    Computes the TF-IDF score for a given word in a document within a collection of documents.\n",
        "\n",
        "    Args:\n",
        "        word (str): The word to compute TF-IDF for.\n",
        "        document (list): A list of words representing the document.\n",
        "        documents (list of list): A list of documents, where each document is a list of words.\n",
        "\n",
        "    Returns:\n",
        "        float: The TF-IDF value for the word in the document.\n",
        "    \"\"\"\n",
        "    tf = compute_tf(word, document)\n",
        "    idf = compute_idf(word, documents)\n",
        "    return tf * idf\n",
        "\n",
        "# Example documents (already defined in previous cells):\n",
        "# doc1 = \"I love NLP NLP\".split()\n",
        "# doc2 = \"NLP loves Python\".split()\n",
        "# all_docs = [doc1, doc2]\n",
        "\n",
        "# Compute TF-IDF for each word in each document using the provided example code\n",
        "for doc_index, doc in enumerate(all_docs):\n",
        "    print(f\"Document {doc_index + 1}:\")\n",
        "    # Use set(doc) to process each unique word in the document\n",
        "    for word in set(doc):\n",
        "        tf_idf_value = compute_tf_idf(word, doc, all_docs)\n",
        "        print(f\"  {word}: {tf_idf_value:.3f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "  I: 0.173\n",
            "  NLP: 0.000\n",
            "  love: 0.173\n",
            "Document 2:\n",
            "  Python: 0.231\n",
            "  NLP: 0.000\n",
            "  loves: 0.231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2b8b33b"
      },
      "source": [
        "### Scikit-learn Bag of Words\n",
        "\n",
        "Manually calculating TF-IDF can be tedious for large collections of documents. Libraries like `scikit-learn` provide efficient tools to do this automatically.\n",
        "\n",
        "First, let's look at how `scikit-learn` can be used to create a **Bag of Words** representation using the `CountVectorizer`. As we discussed earlier, Bag of Words represents text by the frequency of words, ignoring word order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5558914f",
        "outputId": "235b1be3-e5c3-4c81-b3cd-d29a6c27feae"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample sentences (from the provided code):\n",
        "documents = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP loves Python.\"\n",
        "]\n",
        "\n",
        "# Create BoW representation\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# The vectorizer learns the vocabulary and creates the Bag of Words matrix.\n",
        "# Let's see the vocabulary it learned:\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# And here is the Bag of Words matrix, showing word counts for each document:\n",
        "print(\"BoW Matrix:\\n\", X.toarray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['love' 'loves' 'nlp' 'python']\n",
            "BoW Matrix:\n",
            " [[1 0 1 0]\n",
            " [0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfa4efb"
      },
      "source": [
        "### Scikit-learn TF-IDF\n",
        "\n",
        "`scikit-learn` also provides a convenient class specifically for calculating TF-IDF: `TfidfVectorizer`. This vectorizer combines the steps of `CountVectorizer` (tokenizing and counting) and the TF-IDF calculation into a single process.\n",
        "\n",
        "It calculates the TF-IDF score for each term in each document within the corpus.\n",
        "\n",
        "Let's look at the code provided to use `TfidfVectorizer`.\n",
        "\n",
        "First, you need to import `TfidfVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca3436e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b90ff27"
      },
      "source": [
        "Then, you initialize `TfidfVectorizer` and fit it to your documents, similar to how you used `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3784b8b1",
        "outputId": "82ef7c14-5d86-4cf8-aa72-5109b443ad15"
      },
      "source": [
        "# Sample sentences (from the provided code):\n",
        "documents = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP loves Python.\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# The vectorizer learns the vocabulary and computes the TF-IDF matrix.\n",
        "# You can see the vocabulary it learned (which is the same as with CountVectorizer for this example):\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# And here is the TF-IDF matrix:\n",
        "# The values are the TF-IDF scores for each word in each document.\n",
        "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['love' 'loves' 'nlp' 'python']\n",
            "TF-IDF Matrix:\n",
            " [[0.81480247 0.         0.57973867 0.        ]\n",
            " [0.         0.6316672  0.44943642 0.6316672 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd41910b"
      },
      "source": [
        "### Computing Document Similarity using Cosine Similarity\n",
        "\n",
        "Once we have the TF-IDF matrix representing our documents, we can compute the similarity between any two documents. A common and effective measure for this is **Cosine Similarity**.\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two non-zero vectors. In the context of text analysis, the vectors are the TF-IDF vectors of the documents.\n",
        "\n",
        "*   If the angle between the vectors is 0 degrees, the cosine similarity is 1, indicating the documents are identical in content.\n",
        "*   If the angle is 90 degrees, the cosine similarity is 0, indicating the documents have no words in common.\n",
        "*   Values between 0 and 1 indicate varying degrees of similarity.\n",
        "\n",
        "The formula for cosine similarity between two vectors A and B is:\n",
        "\n",
        "$$ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} $$\n",
        "\n",
        "Where:\n",
        "*   $A \\cdot B$ is the dot product of vectors A and B.\n",
        "*   $||A||$ and $||B||$ are the magnitudes (or Euclidean norms) of vectors A and B.\n",
        "\n",
        "Fortunately, `scikit-learn` provides a convenient function to compute cosine similarity directly from the TF-IDF matrix.\n",
        "\n",
        "Let's add code to compute the cosine similarity between our two example documents using the TF-IDF matrix we generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea8013bb",
        "outputId": "5ca41e33-e0d8-4ce2-a7e8-d0b407a8a611"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# We use the TF-IDF matrix X_tfidf that we computed in the previous step.\n",
        "# Each row in X_tfidf represents a document.\n",
        "# X_tfidf.toarray() converts the sparse matrix to a dense numpy array.\n",
        "# The cosine_similarity function can compute the similarity between all pairs of rows in the matrix.\n",
        "similarity_matrix = cosine_similarity(X_tfidf)\n",
        "\n",
        "# The result is a matrix where entry (i, j) is the cosine similarity between document i and document j.\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(similarity_matrix)\n",
        "\n",
        "# To get the similarity between the first document (index 0) and the second document (index 1):\n",
        "similarity_doc1_doc2 = similarity_matrix[0, 1]\n",
        "print(f\"\\nCosine Similarity between Document 1 and Document 2: {similarity_doc1_doc2:.3f}\")\n",
        "\n",
        "# Note that the diagonal elements (i, i) are always 1, as a document is perfectly similar to itself.\n",
        "# The matrix is also symmetric (similarity_matrix[i, j] is the same as similarity_matrix[j, i])."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.26055567]\n",
            " [0.26055567 1.        ]]\n",
            "\n",
            "Cosine Similarity between Document 1 and Document 2: 0.261\n"
          ]
        }
      ]
    }
  ]
}